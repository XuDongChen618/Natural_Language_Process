import csv
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from gensim.models import Word2Vec
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict
import warnings
import os
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR

# 忽略警告信息
warnings.filterwarnings("ignore")

# 超参数设置
EMBEDDING_DIM = 512
HIDDEN_DIM = 256
CNN_OUT_CHANNELS = 128
CNN_KERNEL_SIZES = [3, 4, 5, 6, 7]
NUM_CLASSES = 14
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 0.0003
MAX_SEQ_LENGTH = 128
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_HEADS = 4
GRADIENT_ACCUMULATION_STEPS = 4

# 标签与类别的对应关系
label2id = {
    '科技': 0, '股票': 1, '体育': 2, '娱乐': 3,
    '时政': 4, '社会': 5, '教育': 6, '财经': 7,
    '家居': 8, '游戏': 9, '房产': 10, '时尚': 11,
    '彩票': 12, '星座': 13
}


# 创建DataLoader
class NewsDataset(Dataset):
    def __init__(self, df, vocab, max_seq_length, is_test=False):
        self.df = df
        self.is_test = is_test
        self.vocab = vocab
        self.max_seq_length = max_seq_length
        self.data = []
        self.labels = []
        self.texts = []  # 保存原始文本用于伪标签
        self.process_data()

    def process_data(self):
        for _, row in self.df.iterrows():
            if self.is_test:
                # 处理测试集
                text = row['text']
                label = None
            else:
                # 处理训练集
                label = int(row['label'])
                text = row['text']

            # 保存原始文本
            self.texts.append(text)

            # 将文本转换为字符级索引序列
            char_seq = [self.vocab[c] if c in self.vocab else self.vocab['<UNK>'] for c in text.strip()]

            # 截断或填充到固定长度
            if len(char_seq) > self.max_seq_length:
                char_seq = char_seq[:self.max_seq_length]
            else:
                char_seq += [self.vocab['<PAD>']] * (self.max_seq_length - len(char_seq))

            self.data.append(char_seq)

            if not self.is_test:
                self.labels.append(label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if self.is_test:
            return torch.LongTensor(self.data[idx]), self.texts[idx]
        else:
            return torch.LongTensor(self.data[idx]), self.labels[idx]


# 构建词汇表
def build_vocab(df, min_freq=1):
    vocab = defaultdict(int)
    for _, row in df.iterrows():
        text = row['text']
        for char in text:
            vocab[char] += 1

    # 添加特殊字符
    vocab_list = ['<PAD>', '<UNK>']
    # 添加所有出现过的字符，不管频率高低
    for char in vocab:
        vocab_list.append(char)

    # 创建字符到索引的映射
    vocab = {char: idx for idx, char in enumerate(vocab_list)}

    return vocab


# 训练词向量
def train_word2vec(df, vocab, embed_dim=EMBEDDING_DIM, window=5, min_count=1, sg=1):
    # 准备训练数据
    sentences = []
    for _, row in df.iterrows():
        text = row['text']
        sentences.append([char for char in text])

    # 训练Word2Vec模型
    model = Word2Vec(sentences, vector_size=embed_dim, window=window, min_count=min_count, sg=sg, workers=4)

    # 创建词向量矩阵
    vocab_size = len(vocab)
    embedding_matrix = np.zeros((vocab_size, embed_dim))

    for char, idx in vocab.items():
        if char in model.wv:
            embedding_matrix[idx] = model.wv[char]
        else:
            # 随机初始化未出现在训练集中的词
            embedding_matrix[idx] = np.random.normal(scale=0.1, size=(embed_dim,))

    return embedding_matrix


# 多头注意力机制
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        assert self.head_dim * num_heads == embed_dim, "Embedding dimension must be divisible by number of heads"

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.fc_out = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # 线性变换
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # 重塑为多头
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # 计算注意力分数
        energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention = torch.softmax(energy, dim=-1)

        # 应用注意力权重
        out = torch.matmul(attention, V)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)

        # 最终线性变换
        out = self.fc_out(out)
        return out


# 标签平滑交叉熵损失
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing

    def forward(self, x, target):
        confidence = 1. - self.smoothing
        logprobs = F.log_softmax(x, dim=-1)
        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)
        smooth_loss = -logprobs.mean(dim=-1)
        loss = confidence * nll_loss + self.smoothing * smooth_loss
        return loss.mean()


# 定义CNN + 多头注意力 + LSTM模型
class CNN_MultiheadAttention_LSTM_Model(nn.Module):
    def __init__(self, vocab_size, embedding_dim, cnn_out_channels, cnn_kernel_sizes, hidden_dim, num_classes,
                 num_heads):
        super(CNN_MultiheadAttention_LSTM_Model, self).__init__()

        # embedding层
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # CNN层 - 使用相同的padding保持序列长度
        self.convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(in_channels=embedding_dim,
                          out_channels=cnn_out_channels,
                          kernel_size=ks,
                          padding=ks // 2),
                nn.ReLU(),
                nn.MaxPool1d(kernel_size=2)  # 添加池化层减少序列长度
            )
            for ks in cnn_kernel_sizes
        ])

        # 计算CNN输出维度
        self.cnn_output_dim = cnn_out_channels * len(cnn_kernel_sizes)

        # 自适应池化层，确保所有CNN输出具有相同的长度
        self.adaptive_pool = nn.AdaptiveAvgPool1d(MAX_SEQ_LENGTH // 4)  # 减少序列长度

        # 多头注意力层
        self.multihead_attention = MultiHeadAttention(
            embed_dim=self.cnn_output_dim,
            num_heads=num_heads
        )

        # LSTM层
        self.lstm = nn.LSTM(input_size=self.cnn_output_dim,
                            hidden_size=hidden_dim,
                            num_layers=1,
                            batch_first=True,
                            bidirectional=False,
                            dropout=0.3)

        # 注意力机制
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

        # 全连接层
        self.fc = nn.Linear(hidden_dim, num_classes)

        # 层归一化
        self.layer_norm1 = nn.LayerNorm(self.cnn_output_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)

        # Dropout
        self.dropout = nn.Dropout(0.5)  # 增加Dropout率以防止过拟合

        # BatchNorm层
        self.batch_norm1 = nn.BatchNorm1d(embedding_dim)
        self.batch_norm2 = nn.BatchNorm1d(self.cnn_output_dim)

        # 损失函数 - 添加标签平滑
        self.loss_fn = LabelSmoothingCrossEntropy(smoothing=0.1)

    def forward(self, x):
        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        x = x.permute(0, 2, 1)  # [batch_size, embed_dim, seq_len]

        # CNN特征提取
        conv_outs = []
        for conv in self.convs:
            conv_out = conv(x)  # [batch_size, cnn_out_channels, seq_len//2]
            conv_out = self.adaptive_pool(conv_out)  # [batch_size, cnn_out_channels, MAX_SEQ_LENGTH//4]
            conv_outs.append(conv_out)

        # 拼接CNN输出，在通道维度
        cnn_out = torch.cat(conv_outs,
                            dim=1)  # [batch_size, cnn_out_channels * len(cnn_kernel_sizes), MAX_SEQ_LENGTH//4]

        # 转置为 [batch_size, seq_len, features] 用于多头注意力
        cnn_out = cnn_out.permute(0, 2, 1)  # [batch_size, MAX_SEQ_LENGTH//4, features]

        # 应用层归一化
        cnn_out = self.layer_norm1(cnn_out)

        # 多头注意力处理 
        attention_out = self.multihead_attention(cnn_out)  # [batch_size, seq_len, features]

        # 残差连接
        attention_out = attention_out + cnn_out

        # 应用Dropout
        attention_out = self.dropout(attention_out)

        # LSTM处理
        lstm_out, _ = self.lstm(attention_out)  # [batch_size, seq_len, hidden_dim]

        # 应用层归一化
        lstm_out = self.layer_norm2(lstm_out)

        # 注意力机制
        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]
        attention_weights = torch.softmax(attention_weights, dim=1)  # [batch_size, seq_len, 1]

        # 加权求和
        attention_out = torch.sum(lstm_out * attention_weights, dim=1)  # [batch_size, hidden_dim]

        # 应用Dropout
        attention_out = self.dropout(attention_out)

        # 分类
        logits = self.fc(attention_out)
        return logits

    def compute_loss(self, logits, labels):
        return self.loss_fn(logits, labels)


# 验证模型并统计每个类别的预测数量
def validate_model(model, val_loader):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    class_counts = {cls: {'total': 0, 'correct': 0} for cls in range(NUM_CLASSES)}

    with torch.no_grad():
        for batch in val_loader:
            inputs, labels = batch
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = model(inputs)
            loss = model.compute_loss(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # 统计每个类别的预测数量和正确数量
            for label, pred in zip(labels, predicted):
                label = label.item()
                pred = pred.item()
                class_counts[label]['total'] += 1
                if label == pred:
                    class_counts[label]['correct'] += 1

    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total
    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')

    # 打印每个类别的预测结果
    print("\nValidation results per class:")
    for cls in range(NUM_CLASSES):
        total_samples = class_counts[cls]['total']
        correct_samples = class_counts[cls]['correct']
        if total_samples > 0:
            class_accuracy = correct_samples / total_samples
            print(f'Class {cls} ({list(label2id.keys())[list(label2id.values()).index(cls)]}): '
                  f'Total = {total_samples}, Correct = {correct_samples}, Accuracy = {class_accuracy:.4f}')
        else:
            print(f'Class {cls} ({list(label2id.keys())[list(label2id.values()).index(cls)]}): No samples')

    return avg_loss, accuracy


# 训练模型
def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs):
    best_accuracy = 0.0
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    # 早停参数
    patience = 5
    best_val_loss = float('inf')
    epochs_no_improve = 0

    for epoch in range(num_epochs):
        # 训练
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        # 梯度累积计数器
        accumulation_steps = 0

        for i, batch in enumerate(train_loader):
            inputs, labels = batch
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = model(inputs)
            loss = model.compute_loss(outputs, labels)

            # 梯度累积
            loss = loss / GRADIENT_ACCUMULATION_STEPS
            loss.backward()

            accumulation_steps += 1

            # 达到累积步数时更新参数
            if accumulation_steps % GRADIENT_ACCUMULATION_STEPS == 0:
                # 梯度裁剪
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()
                accumulation_steps = 0

            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        # 处理剩余的梯度
        if accumulation_steps > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()

        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')

        train_losses.append(avg_loss)
        train_accuracies.append(accuracy)

        # 验证
        val_loss, val_accuracy = validate_model(model, val_loader)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # 学习率调度
        if isinstance(scheduler, ReduceLROnPlateau):
            scheduler.step(val_loss)
        else:
            scheduler.step()

        # 早停机制
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print(f'Early stopping after {epoch + 1} epochs')
                break

        # 保存最佳模型
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), 'best_model.pth')
            print("Best model saved!")

    # 绘制训练曲线
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies, label='Training Accuracy')
    plt.plot(val_accuracies, label='Validation Accuracy')
    plt.title('Accuracy over epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.close()

    return model


# 测试模型
def test_model(model, test_loader):
    model.eval()
    predictions = []

    with torch.no_grad():
        for inputs, texts in test_loader:  # 修改为同时接收文本
            inputs = inputs.to(DEVICE)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            predictions.extend(predicted.cpu().numpy())

    return predictions


# 将预测结果保存到文件
def save_predictions(predictions, output_file):
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        for pred in predictions:
            writer.writerow([pred])


# 随机过采样函数
def random_over_sampling(df):
    # 计算每个类别的样本数量
    class_counts = df['label'].value_counts()
    max_count = class_counts.max()

    # 对每个少数类进行过采样
    df_list = []
    for cls in range(NUM_CLASSES):
        class_df = df[df['label'] == cls]
        # 计算需要复制的次数
        num_samples = max_count - len(class_df)
        if num_samples > 0:
            # 随机复制样本
            resampled_df = class_df.sample(n=num_samples, replace=True, random_state=42)
            df_list.append(resampled_df)
        df_list.append(class_df)

    # 合并所有类别
    resampled_df = pd.concat(df_list)
    # 打乱数据顺序
    resampled_df = resampled_df.sample(frac=1, random_state=42).reset_index(drop=True)
    return resampled_df


# 伪标签技术
def pseudo_labeling(model, test_dataset, vocab, threshold=0.9):
    model.eval()
    pseudo_data = []

    # 创建测试集的DataLoader
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    with torch.no_grad():
        for inputs, texts in test_loader:
            inputs = inputs.to(DEVICE)
            outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)
            max_probs, preds = torch.max(probs, dim=1)

            # 只选择高置信度的预测作为伪标签
            for i in range(len(texts)):
                if max_probs[i] > threshold:
                    pseudo_data.append({
                        'text': texts[i],
                        'label': preds[i].item()
                    })

    # 转换为DataFrame
    pseudo_df = pd.DataFrame(pseudo_data)
    return pseudo_df


# 合并数据集
def combine_datasets(original_df, pseudo_df):
    return pd.concat([original_df, pseudo_df], ignore_index=True)


# 主函数
def main():
    # 读取数据
    train_df = pd.read_csv('train_set.csv', sep='\t')
    test_df = pd.read_csv('test_a.csv', sep='\t')

    # 划分训练集和验证集
    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

    # 对训练集进行随机过采样，验证集保持原样
    train_df_resampled = random_over_sampling(train_df)

    # 构建词汇表
    vocab = build_vocab(train_df_resampled)

    # 判断词向量文件是否存在
    embedding_matrix_path = f'embedding_matrix_{EMBEDDING_DIM}.npy'
    if os.path.exists(embedding_matrix_path):
        print("加载预训练的词向量...")
        embedding_matrix = np.load(embedding_matrix_path)
        # 检查维度是否匹配
        if embedding_matrix.shape[1] != EMBEDDING_DIM:
            print(f"维度不匹配，重新训练词向量...")
            embedding_matrix = train_word2vec(train_df_resampled, vocab)
            np.save(embedding_matrix_path, embedding_matrix)
    else:
        print("训练词向量...")
        embedding_matrix = train_word2vec(train_df_resampled, vocab)
        np.save(embedding_matrix_path, embedding_matrix)
        print(f"词向量已保存到 {embedding_matrix_path}")

    # 创建数据集和DataLoader
    train_dataset = NewsDataset(train_df_resampled, vocab, MAX_SEQ_LENGTH)
    val_dataset = NewsDataset(val_df, vocab, MAX_SEQ_LENGTH)
    test_dataset = NewsDataset(test_df, vocab, MAX_SEQ_LENGTH, is_test=True)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # 初始化模型，使用预训练词向量
    model = CNN_MultiheadAttention_LSTM_Model(
        vocab_size=len(vocab),
        embedding_dim=EMBEDDING_DIM,
        cnn_out_channels=CNN_OUT_CHANNELS,
        cnn_kernel_sizes=CNN_KERNEL_SIZES,
        hidden_dim=HIDDEN_DIM,
        num_classes=NUM_CLASSES,
        num_heads=NUM_HEADS
    ).to(DEVICE)

    # 加载预训练的词向量到embedding层
    model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))

    # 定义优化器
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)

    # 使用ReduceLROnPlateau学习率调度
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)

    # 训练模型
    model = train_model(model, train_loader, val_loader, optimizer, scheduler, EPOCHS)

    # 使用伪标签进行第二轮训练
    print("开始伪标签训练...")
    pseudo_df = pseudo_labeling(model, test_dataset, vocab, threshold=0.9)

    if len(pseudo_df) > 0:
        print(f"生成了 {len(pseudo_df)} 个伪标签样本")
        # 合并原始训练数据和伪标签数据
        augmented_train_df = combine_datasets(train_df_resampled, pseudo_df)

        # 创建新的训练数据集
        augmented_train_dataset = NewsDataset(augmented_train_df, vocab, MAX_SEQ_LENGTH)
        augmented_train_loader = DataLoader(augmented_train_dataset, batch_size=BATCH_SIZE, shuffle=True)

        # 使用扩充后的数据重新训练
        model = train_model(model, augmented_train_loader, val_loader, optimizer, scheduler, EPOCHS // 2)
    else:
        print("没有生成足够置信度的伪标签样本，跳过伪标签训练")

    # 测试模型
    predictions = test_model(model, test_loader)

    # 保存预测结果为序号
    save_predictions(predictions, 'test_predictions.csv')
    print("预测结果已保存到 test_predictions.csv")


if __name__ == '__main__':
    main()
