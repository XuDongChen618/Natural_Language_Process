import csv
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from gensim.models import Word2Vec
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict
import warnings
import os
from sklearn.model_selection import train_test_split

# 忽略警告信息
warnings.filterwarnings("ignore")

# 超参数设置
EMBEDDING_DIM = 384  # 词向量维度
HIDDEN_DIM = 256  # LSTM隐藏层维度
CNN_OUT_CHANNELS = 384
CNN_KERNEL_SIZES = [3, 4, 5, 6, 7]  # CNN卷积核大小
NUM_CLASSES = 14  # 分类类别数
BATCH_SIZE = 128
EPOCHS = 10
LEARNING_RATE = 0.001
MAX_SEQ_LENGTH = 512  # 最大序列长度
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 标签与类别的对应关系
label2id = {
    '科技': 0, '股票': 1, '体育': 2, '娱乐': 3,
    '时政': 4, '社会': 5, '教育': 6, '财经': 7,
    '家居': 8, '游戏': 9, '房产': 10, '时尚': 11,
    '彩票': 12, '星座': 13
}


# 创建DataLoader
class NewsDataset(Dataset):
    def __init__(self, df, vocab, max_seq_length, is_test=False):
        self.df = df
        self.is_test = is_test
        self.vocab = vocab
        self.max_seq_length = max_seq_length
        self.data = []
        self.labels = []
        self.process_data()

    def process_data(self):
        for _, row in self.df.iterrows():
            if self.is_test:
                # 处理测试集
                text = row['text']
                label = None
            else:
                # 处理训练集
                label = int(row['label'])
                text = row['text']

            # 将文本转换为字符级索引序列
            char_seq = [self.vocab[c] if c in self.vocab else self.vocab['<UNK>'] for c in text.strip()]

            # 截断或填充到固定长度
            if len(char_seq) > self.max_seq_length:
                char_seq = char_seq[:self.max_seq_length]
            else:
                char_seq += [self.vocab['<PAD>']] * (self.max_seq_length - len(char_seq))

            self.data.append(char_seq)

            if not self.is_test:
                self.labels.append(label)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if self.is_test:
            return torch.LongTensor(self.data[idx])
        else:
            return torch.LongTensor(self.data[idx]), self.labels[idx]


# 构建词汇表
def build_vocab(df, min_freq=1):
    vocab = defaultdict(int)
    for _, row in df.iterrows():
        text = row['text']
        for char in text:
            vocab[char] += 1

    # 添加特殊字符
    vocab_list = ['<PAD>', '<UNK>']
    # 添加所有出现过的字符，不管频率高低
    for char in vocab:
        vocab_list.append(char)

    # 创建字符到索引的映射
    vocab = {char: idx for idx, char in enumerate(vocab_list)}

    return vocab


# 训练词向量
def train_word2vec(df, vocab, embed_dim=EMBEDDING_DIM, window=5, min_count=1, sg=1):
    # 准备训练数据
    sentences = []
    for _, row in df.iterrows():
        text = row['text']
        sentences.append([char for char in text])

    # 训练Word2Vec模型
    model = Word2Vec(sentences, vector_size=embed_dim, window=window, min_count=min_count, sg=sg, workers=4)

    # 创建词向量矩阵
    vocab_size = len(vocab)
    embedding_matrix = np.zeros((vocab_size, embed_dim))

    for char, idx in vocab.items():
        if char in model.wv:
            embedding_matrix[idx] = model.wv[char]
        else:
            # 随机初始化未出现在训练集中的词
            embedding_matrix[idx] = np.random.normal(scale=0.1, size=(embed_dim,))

    return embedding_matrix


# 定义CNN + LSTM模型，先CNN再LSTM，并在LSTM后加入注意力机制
class CNN_LSTM_Attention_Model(nn.Module):
    def __init__(self, vocab_size, embedding_dim, cnn_out_channels, cnn_kernel_sizes, hidden_dim, num_classes):
        super(CNN_LSTM_Attention_Model, self).__init__()

        # embedding层
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # CNN层
        self.convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(in_channels=embedding_dim,
                          out_channels=cnn_out_channels,
                          kernel_size=ks),
                nn.ReLU(),
                nn.MaxPool1d(kernel_size=MAX_SEQ_LENGTH - ks + 1)
            )
            for ks in cnn_kernel_sizes
        ])

        # LSTM层
        self.lstm = nn.LSTM(input_size=cnn_out_channels * len(cnn_kernel_sizes),
                            hidden_size=hidden_dim,
                            num_layers=5,
                            batch_first=True,
                            dropout=0.5)

        # 注意力机制
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

        # 全连接层
        self.fc = nn.Linear(hidden_dim, num_classes)

        # 损失函数
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, x):
        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]
        x = x.permute(0, 2, 1)  # [batch_size, embed_dim, seq_len]

        # CNN特征提取
        conv_outs = []
        for conv in self.convs:
            conv_out = conv(x)
            conv_outs.append(conv_out)

        # 拼接CNN输出
        cnn_out = torch.cat(conv_outs, dim=1).squeeze(-1)  # [batch_size, cnn_out_channels * len(cnn_kernel_sizes)]

        # 添加时间步维度，使其符合LSTM输入要求
        cnn_out = cnn_out.unsqueeze(1)  # [batch_size, 1, feature_dim]

        # LSTM处理
        lstm_out, _ = self.lstm(cnn_out)  # [batch_size, seq_len, hidden_dim]

        # 注意力机制
        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]
        attention_weights = torch.softmax(attention_weights, dim=1)  # [batch_size, seq_len, 1]
        attention_out = lstm_out * attention_weights  # [batch_size, seq_len, hidden_dim]

        # 取最后一个时间步的输出
        attention_out = attention_out[:, -1, :]  # [batch_size, hidden_dim]

        # 分类
        logits = self.fc(attention_out)
        return logits

    def compute_loss(self, logits, labels):
        return self.loss_fn(logits, labels)


# 验证模型并统计每个类别的预测数量
def validate_model(model, val_loader):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    class_counts = {cls: {'total': 0, 'correct': 0} for cls in range(NUM_CLASSES)}

    with torch.no_grad():
        for batch in val_loader:
            inputs, labels = batch
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = model(inputs)
            loss = model.compute_loss(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # 统计每个类别的预测数量和正确数量
            for label, pred in zip(labels, predicted):
                label = label.item()
                pred = pred.item()
                class_counts[label]['total'] += 1
                if label == pred:
                    class_counts[label]['correct'] += 1

    avg_loss = total_loss / len(val_loader)
    accuracy = correct / total
    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')

    # 打印每个类别的预测结果
    print("\nValidation results per class:")
    for cls in range(NUM_CLASSES):
        total_samples = class_counts[cls]['total']
        correct_samples = class_counts[cls]['correct']
        if total_samples > 0:
            class_accuracy = correct_samples / total_samples
            print(f'Class {cls} ({list(label2id.keys())[list(label2id.values()).index(cls)]}): '
                  f'Total = {total_samples}, Correct = {correct_samples}, Accuracy = {class_accuracy:.4f}')
        else:
            print(f'Class {cls} ({list(label2id.keys())[list(label2id.values()).index(cls)]}): No samples')

    return avg_loss, accuracy


# 训练模型
def train_model(model, train_loader, val_loader, optimizer, num_epochs):
    best_accuracy = 0.0
    for epoch in range(num_epochs):
        # 训练
        model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch in train_loader:
            inputs, labels = batch
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = model.compute_loss(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')

        # 验证
        val_loss, val_accuracy = validate_model(model, val_loader)

        # 保存最佳模型
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save(model.state_dict(), 'best_model.pth')
            print("Best model saved!")

    return model


# 测试模型
def test_model(model, test_loader):
    model.eval()
    predictions = []

    with torch.no_grad():
        for inputs in test_loader:
            inputs = inputs.to(DEVICE)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            predictions.extend(predicted.cpu().numpy())

    return predictions



# 将预测结果保存到文件
def save_predictions(predictions, output_file):
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        for pred in predictions:
            writer.writerow([pred])


# 随机过采样函数
def random_over_sampling(df):
    # 计算每个类别的样本数量
    class_counts = df['label'].value_counts()
    max_count = class_counts.max()

    # 对每个少数类进行过采样
    df_list = []
    for cls in range(NUM_CLASSES):
        class_df = df[df['label'] == cls]
        # 计算需要复制的次数
        num_samples = max_count - len(class_df)
        if num_samples > 0:
            # 随机复制样本
            resampled_df = class_df.sample(n=num_samples, replace=True, random_state=42)
            df_list.append(resampled_df)
        df_list.append(class_df)

    # 合并所有类别
    resampled_df = pd.concat(df_list)
    # 打乱数据顺序
    resampled_df = resampled_df.sample(frac=1, random_state=42).reset_index(drop=True)
    return resampled_df


# 主函数
def main():
    # 读取数据
    train_df = pd.read_csv('train_set.csv', sep='\t')
    test_df = pd.read_csv('test_a.csv', sep='\t')

    # 划分训练集和验证集
    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

    # 对训练集和验证集分别进行随机过采样
    train_df_resampled = random_over_sampling(train_df)
    val_df_resampled = random_over_sampling(val_df)

    # 构建词汇表
    vocab = build_vocab(train_df_resampled)

    # 判断词向量文件是否存在
    embedding_matrix_path = 'embedding_matrix.npy'
    if os.path.exists(embedding_matrix_path):
        print("加载预训练的词向量...")
        embedding_matrix = np.load(embedding_matrix_path)
    else:
        print("训练词向量...")
        embedding_matrix = train_word2vec(train_df_resampled, vocab)
        np.save(embedding_matrix_path, embedding_matrix)
        print(f"词向量已保存到 {embedding_matrix_path}")

    # 创建数据集和DataLoader
    train_dataset = NewsDataset(train_df_resampled, vocab, MAX_SEQ_LENGTH)
    val_dataset = NewsDataset(val_df_resampled, vocab, MAX_SEQ_LENGTH)
    test_dataset = NewsDataset(test_df, vocab, MAX_SEQ_LENGTH, is_test=True)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

    # 初始化模型，使用预训练词向量
    model = CNN_LSTM_Attention_Model(
        vocab_size=len(vocab),
        embedding_dim=EMBEDDING_DIM,
        cnn_out_channels=CNN_OUT_CHANNELS,
        cnn_kernel_sizes=CNN_KERNEL_SIZES,
        hidden_dim=HIDDEN_DIM,
        num_classes=NUM_CLASSES
    ).to(DEVICE)

    # 加载预训练的词向量到embedding层
    model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))

    # 定义优化器
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 训练模型
    model = train_model(model, train_loader, val_loader, optimizer, EPOCHS)

    # 测试模型
    predictions = test_model(model, test_loader)

    # 保存预测结果为序号
    save_predictions(predictions, 'test_predictions.csv')
    print("预测结果已保存到 test_predictions.csv")


if __name__ == '__main__':
    main()
